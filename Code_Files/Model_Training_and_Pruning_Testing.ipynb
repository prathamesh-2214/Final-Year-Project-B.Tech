{"cells":[{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-03-24T15:10:48.666220Z","iopub.status.busy":"2024-03-24T15:10:48.664558Z","iopub.status.idle":"2024-03-24T15:10:48.696024Z","shell.execute_reply":"2024-03-24T15:10:48.694660Z","shell.execute_reply.started":"2024-03-24T15:10:48.666009Z"}},"source":["# Model Pruning and Training Notebook\n","\n","## Overview\n","This notebook is designed for performing model pruning and training of proposed deep learning architectures, as well as evaluating their pruned versions. It also includes testing the model architectures on a reconstructed dataset and performing metrics comparison.\n","\n","## Objectives\n","- **Model Pruning:** Reduce Model Size as per proposed approach\n","- **Model Training:** Train proposed  deep learning architectures, and evaluate their effectiveness.\n","- **Reconstructed Dataset Testing:** Test the trained models on a reconstructed dataset to assess generalization capabilities.\n","- **Metrics Comparison:** Compare various metrics such as training accuracy (90%), precision (80%), validation accuracy (75% for Lenet-5 architecture), and ensure testing accuracy meets a threshold of 70% with up to 2% error acceptance.\n","\n","## Specifications\n","- **Training Accuracy:** Targeting 90% accuracy during training.\n","- **Precision and Accuracy:** Aim for 80% precision and accuracy metrics.\n","- **Validation Accuracy:** Lenet-5 architecture is expected to achieve 75% accuracy during validation.\n","- **Testing Criteria:** Ensure testing accuracy meets a minimum threshold of 98%, with up to 2% error acceptance.\n","\n","## Usage\n","This notebook serves as a comprehensive tool for deep learning model development and evaluation. It includes:\n","- Data preprocessing steps.\n","- Model architecture design and implementation.\n","- Hyperparameter tuning.\n","- Model pruning techniques.\n","- Training, validation, and testing phases.\n","- Evaluation of metrics and performance analysis.\n","\n","By following the structured workflow in this notebook, users can effectively experiment with different deep learning architectures, optimize model performance through pruning, and benchmark against specified performance metrics.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T05:46:13.960429Z","iopub.status.busy":"2024-06-12T05:46:13.960184Z","iopub.status.idle":"2024-06-12T05:46:18.570278Z","shell.execute_reply":"2024-06-12T05:46:18.569637Z","shell.execute_reply.started":"2024-06-12T05:46:13.960364Z"},"trusted":true},"outputs":[],"source":["# importing the libraries\n","# import the necessary packages and attributes to be used\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import os\n","from glob import glob\n","from PIL import Image\n","import cv2\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras import Sequential\n","from tensorflow.keras import Model\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from tensorflow.keras.layers import (\n","    Dense,\n","    Conv2D,\n","    MaxPooling2D,\n","    Flatten,\n","    BatchNormalization,\n","    Dropout,\n","    concatenate,\n","    ZeroPadding2D\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Read in metadata and Preprocessing Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T05:47:09.567893Z","iopub.status.busy":"2024-06-12T05:47:09.567142Z","iopub.status.idle":"2024-06-12T05:47:09.604446Z","shell.execute_reply":"2024-06-12T05:47:09.603759Z","shell.execute_reply.started":"2024-06-12T05:47:09.567858Z"},"trusted":true},"outputs":[],"source":["# Reading in the dataset for original / reconstructed dataset as per choice\n","csv_path = \"/kaggle/input/metadata-augmented/HAM10000_metadata_350_augmented.csv\"\n","dataset = pd.read_csv(csv_path)\n","dataset.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T05:47:13.335334Z","iopub.status.busy":"2024-06-12T05:47:13.335039Z","iopub.status.idle":"2024-06-12T05:47:13.469542Z","shell.execute_reply":"2024-06-12T05:47:13.468791Z","shell.execute_reply.started":"2024-06-12T05:47:13.335298Z"},"trusted":true},"outputs":[],"source":["# Define the base directory where images are stored\n","base_skin_dir = os.path.join('..', 'input', '350-components-aug-img')\n","\n","# Create a dictionary where the key is the image_id (without extension) and the value is the full path to the image\n","imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n","                     for x in glob(os.path.join(base_skin_dir, '*', '*.[jJpP][pPnNeEgG]*'))}\n","\n","# Update the dataset with the corresponding image paths\n","# Apply a lambda function to the 'image_id' column to map the image_id to its path using the dictionary\n","# Replace '.png' extension from 'image_id' with an empty string before lookup, this is optional and is performed as the image id in augmented dataset for original and reconstruction contains .png extension in image id\n","dataset['path'] = dataset['image_id'].apply(lambda x: imageid_path_dict.get(x.replace('.png', ''), ''))\n","\n","# Display the count of unique values in the 'path' column to ensure paths are correctly assigned\n","dataset['path'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T05:47:14.017739Z","iopub.status.busy":"2024-06-12T05:47:14.017165Z","iopub.status.idle":"2024-06-12T05:47:14.028085Z","shell.execute_reply":"2024-06-12T05:47:14.027374Z","shell.execute_reply.started":"2024-06-12T05:47:14.017699Z"},"trusted":true},"outputs":[],"source":["# List of allowed values for the 'dx' column\n","allowed_values = ['nc', 'mel', 'nv']\n","\n","# To keep only Melanoma, Non Cancerous, Non-Meanoma, and Melocytic Nevi as classes\n","# Update the 'dx' column:\n","# Apply a lambda function to each element in the 'dx' column\n","# If the value is in the allowed_values list, keep it as is; otherwise, replace it with 'others'\n","dataset['dx'] = dataset['dx'].apply(lambda x: x if x in allowed_values else 'others')\n","\n","# Display the count of unique values in the 'dx' column to ensure values have been correctly updated\n","dx_value_counts = dataset['dx'].value_counts()\n","\n","# Print the value counts for verification\n","print(dx_value_counts)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T05:47:15.228355Z","iopub.status.busy":"2024-06-12T05:47:15.228049Z","iopub.status.idle":"2024-06-12T05:47:15.237977Z","shell.execute_reply":"2024-06-12T05:47:15.237219Z","shell.execute_reply.started":"2024-06-12T05:47:15.228320Z"},"trusted":true},"outputs":[],"source":["\n","## To Sample traininig dataset into 4400 samples, 1100 per class\n","## This is performed only once and not be run again\n","\n","def sample_classes(df, column, n_samples=1100, random_state=None):\n","    \"\"\"\n","    Sample n_samples from each class of a specified column in a pandas DataFrame.\n","\n","    Parameters:\n","    df (pd.DataFrame): The DataFrame to sample from.\n","    column (str): The name of the column to group by.\n","    n_samples (int): The number of samples to take from each class. Default is 1000.\n","    random_state (int, optional): The random state for reproducibility. Default is None.\n","\n","    Returns:\n","    pd.DataFrame: A DataFrame containing the sampled rows.\n","    \"\"\"\n","    # Filter out rows where 'path' is an empty string\n","    df = df[df['path'] != \"\"]\n","    sampled_df = df.groupby(column).apply(lambda x: x.sample(n=n_samples, random_state=random_state))\n","    \n","    # Reset the index to get a clean DataFrame\n","    sampled_df = sampled_df.reset_index(drop=True)\n","    \n","    return sampled_df\n","\n","\n","\n","augmented_df = dataset\n","print(f\"Augmented Data Samples: {augmented_df.shape} \\nDistribution: \\n{augmented_df['dx'].value_counts()}\")\n","## Save the training dataset\n","augmented_df.to_csv('training_origina350.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T05:47:17.184165Z","iopub.status.busy":"2024-06-12T05:47:17.183441Z","iopub.status.idle":"2024-06-12T05:47:17.218793Z","shell.execute_reply":"2024-06-12T05:47:17.218098Z","shell.execute_reply.started":"2024-06-12T05:47:17.184125Z"},"trusted":true},"outputs":[],"source":["# Define a dictionary to map lesion type codes to their human-readable names\n","lesion_type_dict = {\n","    'nv': 'Melanocytic nevi',\n","    'mel': 'Melanoma',\n","    'bkl': 'Benign keratosis-like lesions',\n","    'bcc': 'Basal cell carcinoma',\n","    'akiec': 'Actinic keratoses',\n","    'vasc': 'Vascular lesions',\n","    'df': 'Dermatofibroma',\n","    'nc': 'No_Skin_Disease',\n","    'others': 'Others'\n","}\n","\n","# Map the 'dx' column to human-readable names using the lesion_type_dict\n","# The .map method is used with the .get method of the dictionary to ensure any missing keys return None\n","augmented_df['cell_type'] = augmented_df['dx'].map(lesion_type_dict.get)\n","\n","# Convert the human-readable lesion types to categorical codes\n","# pd.Categorical assigns a unique integer code to each category (lesion type) in 'cell_type'\n","augmented_df['cell_type_idx'] = pd.Categorical(augmented_df['cell_type']).codes\n","\n","# Display the first few rows of the dataframe to verify the changes\n","augmented_df_head = augmented_df.head()\n","\n","# Print the first few rows for verification\n","print(augmented_df_head)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T05:47:17.401074Z","iopub.status.busy":"2024-06-12T05:47:17.400402Z","iopub.status.idle":"2024-06-12T05:47:17.406567Z","shell.execute_reply":"2024-06-12T05:47:17.405776Z","shell.execute_reply.started":"2024-06-12T05:47:17.401041Z"},"trusted":true},"outputs":[],"source":["# Extract Category  mapping of the cell\n","category_mapping = dict(enumerate(pd.Categorical(augmented_df['cell_type']).categories))\n","\n","print(\"Category Mapping (cell_type_idx to cell_type):\")\n","print(category_mapping)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T05:47:17.780858Z","iopub.status.busy":"2024-06-12T05:47:17.780316Z","iopub.status.idle":"2024-06-12T05:47:17.804119Z","shell.execute_reply":"2024-06-12T05:47:17.803387Z","shell.execute_reply.started":"2024-06-12T05:47:17.780828Z"},"trusted":true},"outputs":[],"source":["# Replace zero values in the 'age' column with the median age\n","# .loc is used to select rows where 'age' is 0.0, and those values are replaced with the median age of the 'age' column\n","augmented_df['age'].loc[augmented_df['age'] == 0.0] = augmented_df.age.median()\n","\n","# Fill any remaining NaN values in the 'age' column with 55\n","# .fillna is used to replace NaN values with the specified value (55)\n","augmented_df['age'].fillna(55, inplace=True)\n","\n","# Check for any remaining missing values in the dataset\n","# .isna().sum() returns the count of NaN values for each column\n","missing_values_count = augmented_df.isna().sum()\n","\n","# Print the count of missing values for verification\n","print(missing_values_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T05:47:18.484104Z","iopub.status.busy":"2024-06-12T05:47:18.483383Z","iopub.status.idle":"2024-06-12T05:48:55.732024Z","shell.execute_reply":"2024-06-12T05:48:55.731156Z","shell.execute_reply.started":"2024-06-12T05:47:18.484060Z"},"trusted":true},"outputs":[],"source":["# Define the size to which each image should be resized\n","# This size is to be changed as we change image resolution to train the models\n","size = (450, 600)\n","\n","# Load images into the DataFrame\n","# The 'path' column is used to open each image file\n","# Each image is opened using PIL's Image.open, resized to the specified size, and converted to a numpy array\n","augmented_df['image'] = augmented_df['path'].map(lambda x: np.asarray(Image.open(x).resize(size)))\n","\n","# Display the first few rows of the updated DataFrame to verify the changes\n","augmented_df_head = augmented_df.head()\n","\n","# Print the first few rows for verification\n","print(augmented_df_head)"]},{"cell_type":"markdown","metadata":{},"source":["### Splitting the data into test and training sets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T05:48:57.192385Z","iopub.status.busy":"2024-06-12T05:48:57.192160Z","iopub.status.idle":"2024-06-12T05:48:57.215703Z","shell.execute_reply":"2024-06-12T05:48:57.214951Z","shell.execute_reply.started":"2024-06-12T05:48:57.192352Z"},"trusted":true},"outputs":[],"source":["# Inform the user that data processing is starting\n","print(\"[INFO] processing data...\")\n","\n","# Partition the data into training and testing splits\n","# The DataFrame 'augmented_df' is split into features (attributes) and target (images)\n","# 'train_test_split' is used to split the data\n","# The feature columns selected are 'age', 'sex', 'localization', and 'cell_type_idx'\n","# The target column is 'image'\n","# 75% of the data is used for training and the remaining 25% is used for testing\n","# 'test_size=0.25' specifies the proportion of the dataset to include in the test split\n","# 'random_state=4142' ensures reproducibility of the random splitting\n","train_attr, test_attr, train_img, test_img = train_test_split(\n","    augmented_df[['age', 'sex', 'localization', 'cell_type_idx']], \n","    augmented_df['image'], \n","    test_size=0.25, \n","    random_state=4142\n",")\n","\n","# Display the shapes of the resulting splits to verify the partitioning\n","print(f\"Train attributes shape: {train_attr.shape}\")\n","print(f\"Test attributes shape: {test_attr.shape}\")\n","print(f\"Train images shape: {train_img.shape}\")\n","print(f\"Test images shape: {test_img.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T05:48:57.406428Z","iopub.status.busy":"2024-06-12T05:48:57.406204Z","iopub.status.idle":"2024-06-12T05:48:57.415257Z","shell.execute_reply":"2024-06-12T05:48:57.414631Z","shell.execute_reply.started":"2024-06-12T05:48:57.406401Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","import pickle\n","\n","def normalize_and_encode(df, numeric_cols, categorical_cols, scaler_path=None, encoder_path=None):\n","    \"\"\"\n","    Normalize numeric columns and encode categorical columns in the DataFrame.\n","\n","    Parameters:\n","    df (pd.DataFrame): The DataFrame to process.\n","    numeric_cols (list of str): List of column names for numeric features to be standardized.\n","    categorical_cols (list of str): List of column names for categorical features to be label encoded.\n","    scaler_path (str or None): Optional. File path to save StandardScaler object.\n","    encoder_path (str or None): Optional. File path to save LabelEncoder object.\n","\n","    Returns:\n","    pd.DataFrame: The DataFrame with normalized numeric columns and encoded categorical columns.\n","\n","    Raises:\n","    ValueError: If both numeric_cols and categorical_cols are empty.\n","    \"\"\"\n","    \n","    # Check if input lists are empty\n","    if not numeric_cols and not categorical_cols:\n","        raise ValueError(\"Please provide at least one numeric or categorical column name.\")\n","    \n","    # Standardize numeric columns using StandardScaler\n","    if numeric_cols:\n","        scaler = StandardScaler()\n","        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n","        # Save scaler object if path is provided\n","        if scaler_path:\n","            with open(scaler_path, 'wb') as f:\n","                pickle.dump(scaler, f)\n","    \n","    # Label encode categorical columns using LabelEncoder\n","    if categorical_cols:\n","        for col in categorical_cols:\n","            encoder = LabelEncoder()\n","            df[col] = encoder.fit_transform(df[col])\n","            # Save encoder object if path is provided\n","            if encoder_path:\n","                with open(encoder_path, 'wb') as f:\n","                    pickle.dump(encoder, f)\n","    \n","    return df\n","\n","# Example usage\n","df = pd.DataFrame({'age': [25, 35, 45], 'sex': ['male', 'female', 'male'], 'income': [50000, 60000, 70000]})\n","normalized_df = normalize_and_encode(df, numeric_cols=['age', 'income'], categorical_cols=['sex'], scaler_path='scaler.pkl', encoder_path='encoder.pkl')\n","print(normalized_df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T05:48:57.416690Z","iopub.status.busy":"2024-06-12T05:48:57.416393Z","iopub.status.idle":"2024-06-12T05:48:57.439639Z","shell.execute_reply":"2024-06-12T05:48:57.438957Z","shell.execute_reply.started":"2024-06-12T05:48:57.416650Z"},"trusted":true},"outputs":[],"source":["# Normalize and encode training and testing attribute data\n","train_attr = normalize_and_encode(train_attr, ['age'], ['sex','localization'])\n","test_attr = normalize_and_encode(test_attr, ['age'], ['sex','localization'])\n","\n","# Convert target column 'cell_type_idx' to categorical (one-hot encoding) for model training\n","train_y = to_categorical(list(train_attr['cell_type_idx']), 4)\n","test_y = to_categorical(list(test_attr['cell_type_idx']), 4)\n","\n","# Display shapes for verification\n","print(f\"Train attributes shape: {train_attr.shape}\")\n","print(f\"Test attributes shape: {test_attr.shape}\")\n","print(f\"Train images shape: {train_img.shape}\")\n","print(f\"Test images shape: {test_img.shape}\")\n","print(f\"Train labels shape: {train_y.shape}\")\n","print(f\"Test labels shape: {test_y.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Proposed architectures\n","\n","Note - Refer cnn_concepts.ipynb for techincal definitions of concepts"]},{"cell_type":"markdown","metadata":{},"source":["### Lenet-5 based Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T04:05:24.931151Z","iopub.status.busy":"2024-06-12T04:05:24.930401Z","iopub.status.idle":"2024-06-12T04:05:25.072834Z","shell.execute_reply":"2024-06-12T04:05:25.072069Z","shell.execute_reply.started":"2024-06-12T04:05:24.931111Z"},"trusted":true},"outputs":[],"source":["\n","def create_mixed_nn():\n","    \"\"\"\n","    Creates a mixed neural network model combining MLP (text model) and CNN (image model) branches for prediction.\n","\n","    Returns:\n","    tf.keras.Model: A TensorFlow Keras Model instance representing the mixed neural network.\n","    \"\"\"\n","    # Define input shapes and number of classes\n","    input_shape_text = (3,)\n","    input_shape_image = (600, 450, 3)\n","    num_classes = 4\n","\n","    # Text (MLP) Model\n","    text_model = Sequential(name='text_model')\n","    text_model.add(Dense(4, input_shape=input_shape_text, activation=\"relu\"))\n","    text_model.add(Dropout(0.25))\n","    text_model.add(Dense(6, activation=\"relu\"))\n","    text_model.add(Dense(2, activation=\"relu\"))\n","    text_model.add(Dropout(0.25))\n","    text_model.add(Dense(units=num_classes, activation=\"softmax\"))\n","\n","    # Image (CNN) Model\n","    image_model = Sequential(name='image_model')\n","    image_model.add(Conv2D(6, (5, 5), activation='relu', input_shape=input_shape_image))\n","    image_model.add(MaxPooling2D((2, 2)))\n","    image_model.add(BatchNormalization())\n","    image_model.add(Conv2D(16, (5, 5), activation='relu'))\n","    image_model.add(MaxPooling2D((2, 2)))\n","    image_model.add(BatchNormalization())\n","    image_model.add(Flatten())\n","    image_model.add(Dense(84, activation='relu'))\n","    image_model.add(Dense(120, activation='relu'))\n","\n","    # Concatenate Outputs\n","    combined_output = concatenate([text_model.output, image_model.output])\n","\n","    # Final Dense layers for classification\n","    x = Dense(16, activation=\"relu\")(combined_output)\n","    x = Dense(num_classes, activation=\"softmax\")(x)\n","\n","    # Create a model that includes both text and image inputs, and outputs the final classification\n","    mixed_model = tf.keras.Model([text_model.input, image_model.input], x, name='mixed_model')\n","\n","    return mixed_model\n","\n","# Enable eager execution for debugging (optional)\n","tf.config.run_functions_eagerly(True)\n","\n","# Create the mixed data model\n","mixed_data_model = create_mixed_nn()\n","\n","# Compile the model with optimizer, loss function, and metrics\n","mixed_data_model.compile(\n","    optimizer=Adam(learning_rate=0.0002),\n","    loss='categorical_crossentropy',\n","    metrics=['categorical_accuracy', 'Precision', 'AUC'],  # Add additional metrics for evaluation\n",")\n","\n","# Display model summary\n","mixed_data_model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["### Alexnet based Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T05:48:57.449785Z","iopub.status.busy":"2024-06-12T05:48:57.449297Z","iopub.status.idle":"2024-06-12T05:49:00.312271Z","shell.execute_reply":"2024-06-12T05:49:00.311432Z","shell.execute_reply.started":"2024-06-12T05:48:57.449744Z"},"trusted":true},"outputs":[],"source":["def create_mixed_nn():\n","    \"\"\"\n","    Creates a mixed neural network model combining MLP (text model) and deep CNN (image model) branches for prediction.\n","\n","    Returns:\n","    tf.keras.Model: A TensorFlow Keras Model instance representing the mixed neural network.\n","    \"\"\"\n","    # Define input shape for the image model and number of classes\n","    input_shape_image = (600, 450, 3)\n","    num_classes = 4\n","\n","    # Text (MLP) Model\n","    text_model = Sequential(name='text_model')\n","    text_model.add(Dense(4, input_dim=3, activation=\"relu\"))\n","    text_model.add(BatchNormalization())\n","    text_model.add(Dropout(0.25))\n","    text_model.add(Dense(6, activation=\"relu\"))\n","    text_model.add(BatchNormalization())\n","    text_model.add(Dense(2, activation=\"relu\"))\n","    text_model.add(BatchNormalization())\n","    text_model.add(Dropout(0.25))\n","    text_model.add(Dense(units=num_classes, activation=\"softmax\"))\n","\n","    # Image (CNN) Model\n","    model = Sequential(name='image_model')\n","    model.add(Conv2D(64, (5, 5), strides=(2, 2), input_shape=input_shape_image, activation='relu', padding='valid'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((3, 3), padding='valid'))\n","\n","    model.add(ZeroPadding2D((2, 2)))\n","    model.add(Conv2D(192, (5, 5), strides=(1, 1), activation='relu', padding='same'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((3, 3), padding='valid'))\n","\n","    model.add(ZeroPadding2D((1, 1)))\n","    model.add(Conv2D(384, (3, 3), activation='relu', padding='same'))\n","\n","    model.add(ZeroPadding2D((1, 1)))\n","    model.add(Conv2D(384, (3, 3), activation='relu', padding='same'))\n","\n","    model.add(ZeroPadding2D((1, 1)))\n","    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n","    model.add(MaxPooling2D((3, 3), padding='valid'))\n","\n","    model.add(Flatten())\n","    model.add(Dense(1024, activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(1024, activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(512, activation='relu'))\n","    model.add(Dropout(0.5))\n","\n","    # Concatenate Outputs\n","    combined_output = concatenate([text_model.output, model.output])\n","\n","    # Final Dense layers for classification\n","    x = Dense(8, activation=\"relu\")(combined_output)\n","    x = Dense(num_classes, activation=\"softmax\")(x)\n","\n","    # Create a model that includes both text and image inputs, and outputs the final classification\n","    mixed_model = Model([text_model.input, model.input], x, name='mixed_model')\n","\n","    return mixed_model\n","\n","# Enable eager execution for debugging (optional)\n","tf.config.run_functions_eagerly(True)\n","\n","# Create the mixed data model\n","mixed_data_model = create_mixed_nn()\n","\n","# Compile the model with optimizer, loss function, and metrics\n","mixed_data_model.compile(\n","    optimizer=Adam(learning_rate=0.0002),\n","    loss='categorical_crossentropy',\n","    metrics=['categorical_accuracy', 'Precision', 'AUC'],  # Add additional metrics for evaluation\n",")\n","\n","# Display model summary\n","mixed_data_model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["#### Custom Neural Network - 1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def create_mixed_nn():\n","    \"\"\"\n","    Creates a mixed neural network model combining text (MLP) and image (CNN) inputs.\n","\n","    Returns:\n","    model (tf.keras.Model): Compiled Keras model.\n","    \"\"\"\n","    input_shape_1 = (600, 450, 3)  # Input shape for image (CNN) model\n","    num_classes = 4  # Number of output classes\n","\n","    # Text (MLP) Model\n","    text_model = Sequential(name='text_model')\n","    text_model.add(Dense(4, input_dim=3, activation=\"relu\"))\n","    text_model.add(Dropout(0.25))\n","    text_model.add(Dense(6, activation=\"relu\"))\n","    text_model.add(Dense(2, activation=\"relu\"))\n","    text_model.add(Dropout(0.25))\n","    text_model.add(Dense(units=1, activation=\"relu\"))  # Adjust units for specific classes\n","\n","    # Image (CNN) Model\n","    image_model = Sequential(name='image_model')\n","    image_model.add(Conv2D(16, 7, activation=\"relu\", input_shape=input_shape_1))\n","    image_model.add(MaxPooling2D())\n","    image_model.add(BatchNormalization())\n","    image_model.add(Dropout(0.25))\n","\n","    image_model.add(Conv2D(48, 5, activation=\"relu\"))\n","    image_model.add(MaxPooling2D())\n","    image_model.add(BatchNormalization())\n","    image_model.add(Dropout(0.1))\n","\n","    image_model.add(Conv2D(92, 3, activation=\"relu\"))\n","    image_model.add(MaxPooling2D())\n","    image_model.add(BatchNormalization())\n","    image_model.add(Dropout(0.1))\n","\n","    image_model.add(Conv2D(192, 3, activation=\"relu\"))\n","    image_model.add(MaxPooling2D())\n","    image_model.add(BatchNormalization())\n","    image_model.add(Dropout(0.1))\n","\n","    image_model.add(Conv2D(192, 3, activation=\"relu\"))\n","    image_model.add(MaxPooling2D())\n","    image_model.add(BatchNormalization())\n","    image_model.add(Dropout(0.1))\n","\n","    image_model.add(Conv2D(256, 3, activation=\"relu\"))\n","    image_model.add(MaxPooling2D())\n","    image_model.add(BatchNormalization())\n","    image_model.add(Dropout(0.1))\n","\n","    image_model.add(Flatten())\n","    image_model.add(Dense(1024, activation=\"relu\"))\n","    image_model.add(Dropout(0.15))\n","    image_model.add(BatchNormalization())\n","\n","    image_model.add(Dense(1024, activation=\"relu\"))\n","    image_model.add(Dropout(0.15))\n","    image_model.add(BatchNormalization())\n","\n","    image_model.add(Dense(512, activation=\"relu\"))  # Adjust units for specific classes\n","    image_model.add(Dropout(0.1))\n","\n","    # Concatenate Text and Image Models\n","    combined_output = concatenate([text_model.output, image_model.output])\n","    # Concatenation combines the output of the text_model and image_model, preserving their features in a single vector.\n","\n","    # Final Dense Layers\n","    x = Dense(16, activation=\"relu\")(combined_output)\n","    x = Dense(num_classes, activation=\"softmax\")(x)\n","\n","    # Create Model\n","    model = Model([text_model.input, image_model.input], x, name='mixed_model')\n","\n","    return model\n","\n","# Enable eager execution for immediate execution and debugging\n","tf.config.run_functions_eagerly(True)\n","\n","# Create and Compile the Mixed Neural Network Model\n","mixed_data_model = create_mixed_nn()\n","mixed_data_model.compile(\n","    optimizer=Adam(learning_rate=0.0002),\n","    loss='categorical_crossentropy',\n","    metrics=['categorical_accuracy', 'Precision', 'AUC']\n",")\n","\n","# Print Model Summary\n","mixed_data_model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Neural Network - 2"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","def resnet_block(x, filters, kernel_size=3, strides=1):\n","    \"\"\"\n","    ResNet block with optional downsampling.\n","    \n","    Args:\n","    x (tensor): Input tensor.\n","    filters (int): Number of filters for the convolutional layers.\n","    kernel_size (int): Size of the convolutional kernel.\n","    strides (int): Stride size for the convolutional layers.\n","\n","    Returns:\n","    tensor: Output tensor after applying the ResNet block operations.\n","    \"\"\"\n","    shortcut = x\n","    x = Conv2D(filters, kernel_size, strides=strides, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = Conv2D(filters, kernel_size, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    \n","    # Shortcut connection\n","    if strides != 1 or shortcut.shape[-1] != filters:\n","        shortcut = Conv2D(filters, 1, strides=strides, padding='same')(shortcut)\n","        shortcut = BatchNormalization()(shortcut)\n","    \n","    x = Add()([x, shortcut])\n","    x = Activation('relu')(x)\n","    return x\n","\n","def create_mixed_nn_resnet(): \n","    \"\"\"\n","    Creates a mixed neural network model combining text (MLP) and image (CNN) inputs,\n","    including ResNet blocks for the image model.\n","    \n","    Returns:\n","    model (tf.keras.Model): Compiled Keras model.\n","    \"\"\"\n","    input_shape_1 = (100, 200, 3)  # Input shape for image (CNN) model\n","    num_classes = 8\n","    \n","    # Text (MLP) Model\n","    text_model = Sequential()\n","    text_model.add(Dense(4, input_dim=3, activation=\"relu\"))\n","    text_model.add(BatchNormalization())\n","    text_model.add(Dropout(0.25))\n","    \n","    text_model.add(Dense(6, input_dim=3, activation=\"relu\"))\n","    text_model.add(BatchNormalization())\n","    \n","    text_model.add(Dense(2, input_dim=3, activation=\"relu\"))\n","    text_model.add(BatchNormalization())\n","    text_model.add(Dropout(0.25))\n","\n","    text_model.add(Dense(units=1, activation=\"relu\"))  # Adjust units for specific classes\n","\n","    # Image (CNN) Model with ResNet blocks\n","    inputs = Input(shape=input_shape_1)\n","    \n","    # Initial Convolution and Pooling\n","    x = Conv2D(10, 5, padding='same', activation=\"relu\")(inputs)\n","    x = MaxPooling2D()(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.25)(x)\n","    \n","    # Additional Convolution and Pooling Layers\n","    x = Conv2D(11, 7, padding='same', activation=\"relu\")(x)\n","    x = MaxPooling2D()(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.25)(x)\n","    \n","    x = Conv2D(21, 3, padding='same', activation=\"relu\")(x)\n","    x = MaxPooling2D()(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.25)(x)\n","    \n","    x = Conv2D(32, 3, padding='same', activation=\"relu\")(x)\n","    x = MaxPooling2D()(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.25)(x)\n","    \n","    x = Conv2D(128, 3, padding='same', activation=\"relu\")(x)\n","    x = MaxPooling2D()(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.25)(x)\n","    \n","    # Apply ResNet blocks\n","    x = resnet_block(x, filters=128)\n","    x = Conv2D(168, 3, padding='same', activation=\"relu\")(x)\n","    x = MaxPooling2D()(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.25)(x)\n","    \n","    x = resnet_block(x, filters=168)\n","    \n","    # Flatten and fully connected layers\n","    x = Flatten()(x)\n","    x = Dense(338, activation=\"relu\")(x)\n","    x = Dropout(0.15)(x)\n","    x = BatchNormalization()(x)\n","    \n","    x = Dense(338, activation=\"relu\")(x)\n","    x = Dropout(0.1)(x)\n","    \n","    x = Dense(84, activation=\"relu\")(x)\n","    x = Dropout(0.1)(x)\n","    \n","    # Concatenate Text and Image Model Outputs\n","    combined_output = concatenate([text_model.output, x])\n","\n","    # Final Dense Layers for Classification\n","    x = Dense(16, activation=\"relu\")(combined_output)\n","    x = Dense(num_classes, activation=\"softmax\")(x)\n","    \n","    # Create the Model\n","    model = Model([text_model.input, inputs], x)\n","    \n","    return model\n","\n","# Create and Compile the Model\n","mixed_data_model_resnet = create_mixed_nn_resnet()\n","mixed_data_model_resnet.compile(\n","    optimizer=Adam(learning_rate=0.0002),\n","    loss='categorical_crossentropy',\n","    metrics=['categorical_accuracy', 'Precision', 'AUC']\n",")\n","\n","# Print Model Summary\n","mixed_data_model_resnet.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T18:11:37.726370Z","iopub.status.busy":"2024-06-10T18:11:37.726072Z","iopub.status.idle":"2024-06-10T18:11:40.562381Z","shell.execute_reply":"2024-06-10T18:11:40.561625Z","shell.execute_reply.started":"2024-06-10T18:11:37.726337Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import Sequential, Model, Input\n","from tensorflow.keras.layers import (\n","    Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout, concatenate, Add, Activation\n",")\n","from tensorflow.keras.optimizers import Adam\n","\n","# Define the ResNet block\n","def resnet_block(x, filters, kernel_size=3, strides=1):\n","    \"\"\"\n","    ResNet block with optional downsampling.\n","    \n","    Args:\n","    x (tensor): Input tensor.\n","    filters (int): Number of filters for the convolutional layers.\n","    kernel_size (int): Size of the convolutional kernel.\n","    strides (int): Stride size for the convolutional layers.\n","\n","    Returns:\n","    tensor: Output tensor after applying the ResNet block operations.\n","    \"\"\"\n","    shortcut = x\n","    x = Conv2D(filters, kernel_size, strides=strides, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = Conv2D(filters, kernel_size, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    \n","    # Shortcut connection\n","    if strides != 1 or shortcut.shape[-1] != filters:\n","        shortcut = Conv2D(filters, 1, strides=strides, padding='same')(shortcut)\n","        shortcut = BatchNormalization()(shortcut)\n","    \n","    x = Add()([x, shortcut])\n","    x = Activation('relu')(x)\n","    return x\n","\n","# Function to create your mixed CNN model with ResNet blocks\n","def create_mixed_nn_resnet(): \n","    \"\"\"\n","    Creates a mixed neural network model combining text (MLP) and image (CNN) inputs,\n","    including ResNet blocks for the image model.\n","    \n","    Returns:\n","    model (tf.keras.Model): Compiled Keras model.\n","    \"\"\"\n","    input_shape_1 = (100, 200, 3)  # Input shape for image (CNN) model\n","    num_classes = 4\n","    \n","    # Text (MLP) Model\n","    text_model = Sequential()\n","    text_model.add(Dense(4, input_dim=3, activation=\"relu\"))\n","    text_model.add(BatchNormalization())\n","    text_model.add(Dropout(0.25))\n","    \n","    text_model.add(Dense(6, activation=\"relu\"))\n","    text_model.add(BatchNormalization())\n","    \n","    text_model.add(Dense(2, activation=\"relu\"))\n","    text_model.add(BatchNormalization())\n","    text_model.add(Dropout(0.25))\n","\n","    text_model.add(Dense(units=1, activation=\"relu\"))  # Adjust units for specific classes\n","\n","    # Image (CNN) Model with ResNet blocks\n","    inputs = Input(shape=input_shape_1)\n","    \n","    # Initial Convolution and Pooling\n","    x = Conv2D(16, 5, padding='same', activation=\"relu\")(inputs)\n","    x = MaxPooling2D()(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.25)(x)\n","    \n","    # Additional Convolution and Pooling Layers\n","    x = Conv2D(32, 7, padding='same', activation=\"relu\")(x)\n","    x = MaxPooling2D()(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.25)(x)\n","    \n","    x = Conv2D(64, 3, padding='same', activation=\"relu\")(x)\n","    x = MaxPooling2D()(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.25)(x)\n","    \n","    x = Conv2D(96, 3, padding='same', activation=\"relu\")(x)\n","    x = MaxPooling2D()(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.25)(x)\n","    \n","    x = Conv2D(192, 3, padding='same', activation=\"relu\")(x)\n","    x = MaxPooling2D()(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.25)(x)\n","    \n","    # Apply ResNet blocks\n","    x = resnet_block(x, filters=192)\n","    x = Conv2D(256, 3, padding='same', activation=\"relu\")(x)\n","    x = MaxPooling2D()(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.25)(x)\n","    \n","    x = resnet_block(x, filters=256)\n","    \n","    # Flatten and fully connected layers\n","    x = Flatten()(x)\n","    x = Dense(512, activation=\"relu\")(x)\n","    x = Dropout(0.15)(x)\n","    x = BatchNormalization()(x)\n","    \n","    x = Dense(256, activation=\"relu\")(x)\n","    x = Dropout(0.1)(x)\n","    \n","    x = Dense(128, activation=\"relu\")(x)\n","    x = Dropout(0.1)(x)\n","    \n","    # Concatenate Text and Image Model Outputs\n","    combined_output = concatenate([text_model.output, x])\n","\n","    # Final Dense Layers for Classification\n","    x = Dense(8, activation=\"relu\")(combined_output)\n","    x = Dense(num_classes, activation=\"softmax\")(x)\n","    \n","    # Create the Model\n","    model = Model([text_model.input, inputs], x)\n","    \n","    return model\n","\n","# Create the model\n","mixed_data_model_resnet = create_mixed_nn_resnet()\n","\n","# Enable eager execution for debugging (optional)\n","tf.config.run_functions_eagerly(True)\n","\n","# Compile the model\n","mixed_data_model_resnet.compile(\n","    optimizer=Adam(learning_rate=0.0002),\n","    loss='categorical_crossentropy',\n","    metrics=['categorical_accuracy', 'Precision', 'AUC'],  # Add additional metrics\n",")\n","\n","# Print model summary\n","mixed_data_model_resnet.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T05:49:00.313861Z","iopub.status.busy":"2024-06-12T05:49:00.313562Z","iopub.status.idle":"2024-06-12T05:49:00.324887Z","shell.execute_reply":"2024-06-12T05:49:00.324090Z","shell.execute_reply.started":"2024-06-12T05:49:00.313821Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.callbacks import Callback\n","\n","class CustomCheckpoint(Callback):\n","    \"\"\"\n","    Custom Keras callback to save the model based on specified performance thresholds during training.\n","\n","    Attributes:\n","        model (tf.keras.Model): The Keras model to be saved.\n","        save_path (str): The directory path where the model checkpoints will be saved.\n","        monitor_acc (str): The metric to monitor for validation accuracy (default is 'val_categorical_accuracy').\n","        monitor_prec (str): The metric to monitor for validation precision (default is 'val_precision').\n","        monitor_train_acc (str): The metric to monitor for training accuracy (default is 'categorical_accuracy').\n","        threshold (float): The threshold value for validation accuracy and precision above which the model will be saved.\n","        train_threshold (float): The threshold value for training accuracy above which the model will be saved.\n","    \"\"\"\n","\n","    def __init__(self, model, save_path, monitor_acc='val_categorical_accuracy', monitor_prec='val_precision', \n","                 monitor_train_acc='categorical_accuracy', threshold=0.79, train_threshold=0.9):\n","        \"\"\"\n","        Initializes the CustomCheckpoint callback.\n","\n","        Args:\n","            model (tf.keras.Model): The Keras model to be saved.\n","            save_path (str): The directory path where the model checkpoints will be saved.\n","            monitor_acc (str, optional): The metric to monitor for validation accuracy (default is 'val_categorical_accuracy').\n","            monitor_prec (str, optional): The metric to monitor for validation precision (default is 'val_precision').\n","            monitor_train_acc (str, optional): The metric to monitor for training accuracy (default is 'categorical_accuracy').\n","            threshold (float, optional): The threshold value for validation accuracy and precision above which the model will be saved (default is 0.79).\n","            train_threshold (float, optional): The threshold value for training accuracy above which the model will be saved (default is 0.9).\n","        \"\"\"\n","        super(CustomCheckpoint, self).__init__()\n","        self.model = model\n","        self.save_path = save_path\n","        self.monitor_acc = monitor_acc\n","        self.monitor_prec = monitor_prec\n","        self.monitor_train_acc = monitor_train_acc\n","        self.threshold = threshold\n","        self.train_threshold = train_threshold\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        \"\"\"\n","        Called at the end of each epoch during training to check and save the model based on specified thresholds.\n","\n","        Args:\n","            epoch (int): The current epoch number.\n","            logs (dict): Dictionary containing the metrics from the current epoch.\n","        \"\"\"\n","        val_acc = logs.get(self.monitor_acc)\n","        val_prec = logs.get(self.monitor_prec)\n","        train_acc = logs.get(self.monitor_train_acc)\n","        \n","        if val_acc is not None and val_prec is not None and train_acc is not None:\n","            # Check if validation accuracy, precision, and training accuracy meet the thresholds\n","            if val_acc > self.threshold and val_prec > self.threshold and train_acc > self.train_threshold:\n","                print(f'\\nEpoch {epoch + 1}: val_categorical_accuracy, val_precision, and categorical_accuracy are above {self.threshold * 100}% - saving model to {self.save_path}')\n","                # Save the model with detailed metrics in the filename\n","                self.model.save(f\"{self.save_path}_{val_acc:.4f}_{val_prec:.4f}_{train_acc:.4f}.h5\")\n","\n","# Sample usage\n","checkpoint_path = \"alexnet_augmented\"\n","custom_checkpoint = CustomCheckpoint(model=mixed_data_model, save_path=checkpoint_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T05:49:00.326122Z","iopub.status.busy":"2024-06-12T05:49:00.325899Z","iopub.status.idle":"2024-06-12T06:18:59.731703Z","shell.execute_reply":"2024-06-12T06:18:59.729050Z","shell.execute_reply.started":"2024-06-12T05:49:00.326096Z"},"trusted":true},"outputs":[],"source":["from datetime import datetime\n","\n","\n","def export_history_to_csv(history, filename):\n","    \"\"\"\n","    Export the training history to a CSV file.\n","\n","    Args:\n","        history (History): The History object returned from model.fit().\n","        filename (str): The filename to save the CSV file.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    # Convert the history.history dictionary to a DataFrame\n","    history_df = pd.DataFrame(history.history)\n","    \n","    # Save the DataFrame to a CSV file\n","    history_df.to_csv(filename, index=False)\n","    \n","    print(f\"[INFO] Training history exported to {filename}\")\n","\n","print(\"[INFO] Starting model training...\")\n","img_train = np.array(list(train_img))\n","\n","# Get the current time before training starts\n","start_time = datetime.now()\n","\n","# Print the starting time\n","print(\"[INFO] Training started at:\", start_time)\n","\n","# Train the model\n","history = mixed_data_model.fit(\n","    x=[train_attr[['age','sex','localization']], img_train], \n","    y=np.array(train_y),\n","    validation_data=([test_attr[['age','sex','localization']], np.array(list(test_img))], np.array(test_y)),\n","    epochs=75, \n","    batch_size=32,\n","    callbacks=[custom_checkpoint]\n",")\n","\n","# Get the current time after training completes\n","end_time = datetime.now()\n","\n","# Print the ending time\n","print(\"[INFO] Training ended at:\", end_time)\n","\n","# Example usage of export_history_to_csv\n","export_history_to_csv(history, 'ALEXNET_augmented_40min.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T18:08:16.764484Z","iopub.status.busy":"2024-06-10T18:08:16.764174Z","iopub.status.idle":"2024-06-10T18:08:16.972069Z","shell.execute_reply":"2024-06-10T18:08:16.971264Z","shell.execute_reply.started":"2024-06-10T18:08:16.764449Z"},"trusted":true},"outputs":[],"source":["## Use to Save Model in case baseline metrics are not met\n","\n","\n","mixed_data_model_resnet.save(\"resnet_augmented_200_100__REDUCED_0.6_once_73.55_7791_8621.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def plot_metrics(history, metric='auc', title='Metric', y_label='Metric Value', ylim=None):\n","    \"\"\"\n","    Plot the specified metric from the training history.\n","\n","    Args:\n","        history (History): The History object returned from model.fit().\n","        metric (str): The metric to plot from history (e.g., 'auc', 'precision', 'categorical_accuracy').\n","        title (str): The title of the plot.\n","        y_label (str): The label for the y-axis.\n","        ylim (tuple): Optional. Tuple specifying the y-axis limits (min, max).\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    plt.figure(figsize=(12, 6))\n","\n","    # Plot training metric\n","    plt.plot(history.history[metric], label=f'Training {metric.capitalize()}')\n","\n","    # Plot validation metric\n","    val_metric = f'val_{metric}'\n","    plt.plot(history.history[val_metric], label=f'Validation {metric.capitalize()}')\n","\n","    # Set title, labels, and fontsize\n","    plt.title(title, fontsize=16)\n","    plt.xlabel('Epoch', fontsize=16)\n","    plt.ylabel(y_label, fontsize=16)\n","    if ylim:\n","        plt.ylim(ylim)\n","    plt.legend(fontsize=16)\n","    plt.xticks(fontsize=16)\n","    plt.yticks(fontsize=16)\n","    plt.show()\n","\n","# Example usage, can be used to plot any metric in recorded history\n","plot_metrics(history, metric='auc', title='AUC', y_label='AUC', ylim=(0.1, 1.2))\n","plot_metrics(history, metric='precision', title='Precision', y_label='Precision', ylim=(0.1, 1.2))\n","plot_metrics(history, metric='categorical_accuracy', title='Categorical Accuracy', y_label='Categorical Accuracy', ylim=(0.1, 1.2))\n"]},{"cell_type":"markdown","metadata":{},"source":["### Testing Model\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## load testing dataset as per choice - original or reconstructed\n","dataset = pd.read_csv(\"E:\\Final_Year_Project\\Implementation\\Image-Text-NN-SC-Detection\\Testing\\ISIC2018_Task3_Test_GroundTruth.csv\")\n","dataset.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","from PIL import Image\n","from glob import glob\n","\n","# Mapping dictionary for disease labels\n","lesion_type_dict = {\n","    'nv': 'Melanocytic nevi',\n","    'mel': 'Melanoma',\n","    'bkl': 'Benign keratosis-like lesions',\n","    'bcc': 'Basal cell carcinoma',\n","    'akiec': 'Actinic keratoses',\n","    'vasc': 'Vascular lesions',\n","    'df': 'Dermatofibroma',\n","    'nc': \"Non_Cancerous\"\n","}\n","\n","def preprocess_dataset(dataset, base_skin_dir):\n","    \"\"\"\n","    Preprocesses the dataset to include human-readable labels, image paths, and resized images.\n","\n","    Parameters:\n","    dataset (pd.DataFrame): The DataFrame containing the dataset.\n","    base_skin_dir (str): Base directory path where image files are stored.\n","\n","    Returns:\n","    pd.DataFrame: Processed DataFrame with added columns for human-readable labels, image paths, and resized images.\n","\n","    Raises:\n","    ValueError: If dataset is empty or base_skin_dir is invalid.\n","    \"\"\"\n","    if dataset.empty:\n","        raise ValueError(\"Dataset is empty.\")\n","    \n","    if not os.path.exists(base_skin_dir):\n","        raise ValueError(f\"Directory '{base_skin_dir}' does not exist.\")\n","    \n","    # Map dx codes to human-readable labels\n","    dataset['cell_type'] = dataset['dx'].map(lesion_type_dict.get) \n","    dataset['disease_label'] = pd.Categorical(dataset['cell_type']).codes\n","    \n","    # Define image path dictionary\n","    imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n","                         for x in glob(os.path.join(base_skin_dir, '*', '*.jp*g'))}\n","    \n","    # Ensure 'image_id' is string type\n","    dataset['image_id'] = dataset['image_id'].astype(str)\n","    \n","    # Map image_id to image path\n","    dataset['path'] = dataset['image_id'].apply(lambda x: imageid_path_dict.get(x.replace('.png', ''), ''))\n","    \n","    # Drop rows with NaN values\n","    dataset.dropna(inplace=True)\n","    \n","    # Resize and load images into 'image' column\n","    dataset['image'] = dataset['path'].map(lambda x: np.asarray(Image.open(x).resize((200, 200))))\n","    \n","    return dataset\n","\n","# Example usage\n","base_skin_dir = os.path.join('..', 'Data')  # Replace with actual base directory path\n","dataset = preprocess_dataset(dataset, base_skin_dir)\n","print(dataset.head())\n","\n","# List of allowed values as in testing data too\n","allowed_values = ['nc', 'mel', 'nv']\n","\n","# Update the dx column with allowed values\n","dataset['dx'] = dataset['dx'].apply(lambda x: x if x in allowed_values else 'others')\n","print(dataset['dx'].value_counts())\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## to sample 100 samples from each class\n","def sample_classes(df, column, n_samples=100, random_state=None):\n","    \"\"\"\n","    Sample n_samples from each class of a specified column in a pandas DataFrame.\n","\n","    Parameters:\n","    df (pd.DataFrame): The DataFrame to sample from.\n","    column (str): The name of the column to group by.\n","    n_samples (int): The number of samples to take from each class. Default is 100.\n","    random_state (int, optional): The random state for reproducibility. Default is None.\n","\n","    Returns:\n","    pd.DataFrame: A DataFrame containing the sampled rows.\n","    \"\"\"\n","    # Filter out rows where 'path' is an empty string\n","    df = df[df['path'] != \"\"]\n","    \n","    # Sample n_samples from each group in the specified column\n","    sampled_df = df.groupby(column).apply(lambda x: x.sample(n=n_samples, random_state=random_state))\n","    \n","    # Reset the index to get a clean DataFrame\n","    sampled_df = sampled_df.reset_index(drop=True)\n","    \n","    return sampled_df\n","\n","# Example usage\n","# Assuming 'dataset' is your original DataFrame and 'dx' is the column to sample from\n","augmented_df = sample_classes(dataset, 'dx', n_samples=100)\n","print(f\"Augmented Data Samples: {augmented_df.shape} \\nDistribution: \\n{augmented_df['dx'].value_counts()}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the scaler object from the pickle file\n","with open('scaler.pkl', 'rb') as f:\n","    scaler = pickle.load(f)\n","# Load the encoder object for localization from the pickle file\n","with open('encoder_localization.pkl', 'rb') as f:\n","    localization_encoder = pickle.load(f)\n","# Load the encoder object for sex from the pickle file\n","with open('encoder_sex.pkl', 'rb') as f:\n","    sex_encoder = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Preprocess the testing data\n","augmented_df['age'] = scaler.transform(augmented_df[['age']])\n","augmented_df['sex'] = sex_encoder.transform(augmented_df[['sex']])\n","augmented_df['localization'] = localization_encoder.transform(augmented_df[['localization']])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# partition the \n","# the data  90% for testing, 90 images form each class\n","print(\"[INFO] processing data...\")\n","train_attr, test_attr, train_img, test_img = train_test_split(augmented_df[['age','sex', 'localization', 'cell_type_idx']], augmented_df['image'], test_size=0.9, random_state=4142)\n","\n","## One hot encoding of variables\n","train_y = to_categorical(list(train_attr['cell_type_idx']),4)\n","test_y = to_categorical(list(test_attr['cell_type_idx']),4)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","import h5py\n","import tensorflow as tf\n","from sklearn.metrics import classification_report, recall_score, precision_score, f1_score, accuracy_score\n","\n","# Define input and output directories\n","modelname = \"Custom_CNN\"\n","input_dir = f'E:/Final_Year_Project/Models/{modelname}'\n","output_dir = f'E:/Final_Year_Project/Models/{modelname}/Analysis'\n","\n","# Ensure the output directory exists\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Iterate through each file in the input directory\n","for filename in os.listdir(input_dir):\n","    # Check if the filename contains 'original_200_200' and ends with '.h5'\n","    if 'original_200_200' in filename and filename.endswith('.h5'):\n","        print(f\"Evaluating Model: {filename}\")\n","        model_path = os.path.join(input_dir, filename)\n","        \n","        # Load the model\n","        model = tf.keras.models.load_model(model_path)\n","        \n","        # Prepare test data (replace with your own data loading and preprocessing)\n","        y_pred = model.predict([test_attr[['age','sex','localization']], np.array(list(test_img))])\n","        y_pred = np.argmax(y_pred, axis=1)\n","        y_test = np.argmax(np.array(test_y), axis=1)\n","        \n","        # Calculate evaluation metrics\n","        recall = recall_score(y_test, y_pred, average='macro')\n","        precision = precision_score(y_test, y_pred, average='macro')\n","        f1_score_model = f1_score(y_test, y_pred, average='macro')\n","        accuracy_score1 = accuracy_score(y_test, y_pred)\n","        \n","        # Generate classification report\n","        report = classification_report(y_test, y_pred, output_dict=True)\n","        \n","        # Convert report to DataFrame\n","        df_report = pd.DataFrame(report).transpose()\n","        \n","        # Create DataFrame for overall metrics\n","        overall_metrics = pd.DataFrame({\n","            'Metric': ['Recall', 'Precision', 'F1_Score', 'Accuracy'],\n","            'Value': [recall, precision, f1_score_model, accuracy_score1]\n","        })\n","        \n","        # Combine overall metrics with classification report\n","        combined_df = pd.concat([overall_metrics, df_report])\n","        \n","        # Save evaluation results to CSV\n","        output_filename = f\"{filename.replace('.h5', '')}_Testing.csv\"\n","        combined_df.to_csv(os.path.join(output_dir, output_filename))\n","        ## This stores classification report in an output csv file for all models that are being eva;uated\n","        print(f\"Evaluation results saved to: {os.path.join(output_dir, output_filename)}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Final Metrics\n","\n","**Note:** Detailed evaluation metrics and results can be viewed in the \"Results and Discussion\" section of the project report. For comprehensive insights into model performance, please refer to the complete analysis provided in the report.\n","\n","*Thank you for your interest and understanding.*\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5050364,"sourceId":8470013,"sourceType":"datasetVersion"},{"datasetId":5051293,"sourceId":8471236,"sourceType":"datasetVersion"},{"datasetId":5051327,"sourceId":8471280,"sourceType":"datasetVersion"},{"datasetId":5051425,"sourceId":8471414,"sourceType":"datasetVersion"},{"datasetId":5175460,"sourceId":8641651,"sourceType":"datasetVersion"},{"datasetId":5175825,"sourceId":8642202,"sourceType":"datasetVersion"}],"dockerImageVersionId":30140,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"}},"nbformat":4,"nbformat_minor":4}
